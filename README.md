
# Simple Flask RAG Application

This application demonstrates a Retrieval Augmented Generation (RAG) pipeline using Flask and LangChain. It enables users to query a local text document (`data.txt`) and receive responses generated by a Large Language Model (LLM), enhanced with context retrieved from the document.

## Overview

### What is RAG?

Retrieval Augmented Generation (RAG) is a technique designed to improve the factual accuracy and contextual relevance of responses from LLMs. Instead of relying solely on pre-trained knowledge, RAG retrieves relevant information from an external knowledge base—in this case, a local `data.txt` file—and provides this context to the LLM alongside the user’s query. The LLM then generates a response informed by the additional information.

**RAG Workflow:**

1. **Indexing:** The text is split into chunks and vectorized for efficient retrieval.
2. **Retrieval:** Relevant text chunks are retrieved based on the user query.
3. **Augmentation:** Retrieved context is combined with the user query.
4. **Generation:** The LLM uses this combined input to generate a response.


## Hybrid Search Approach

The application uses a hybrid retrieval strategy through LangChain’s `EnsembleRetriever`, which merges the benefits of:

### 1. Semantic Search (Dense Retrieval)

* **Mechanism:** Embeds document chunks and queries using `SentenceTransformerEmbeddings` (`all-MiniLM-L6-v2`).
* **Storage:** Embeddings are stored in a `FAISS` vector store.
* **Strength:** Retrieves conceptually relevant content, even without exact keyword matches.

### 2. Keyword Search (Sparse Retrieval - BM25)

* **Mechanism:** Uses the `BM25Retriever` for keyword-based search.
* **Algorithm:** BM25 ranks documents based on term frequency, inverse document frequency, and document length normalization.
* **Strength:** Highly effective for matching specific terms, names, or codes.

### BM25 Scoring Formula

$$
\text{score}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
$$

Where:

* $\text{IDF}(q_i)$: Inverse Document Frequency
* $f(q_i, D)$: Frequency of term $q_i$ in document $D$
* $|D|$: Length of document $D$
* $\text{avgdl}$: Average document length
* $k_1$: Term frequency scaling parameter (commonly 1.2–2.0)
* $b$: Document length normalization parameter (commonly 0.75)


## Search Weighting

Both retrieval methods are equally weighted:

```python
ensemble_retriever = EnsembleRetriever(
    retrievers=[retriever_vectordb, keyword_retriever],
    weights=[0.5, 0.5]
)
```

These weights can be adjusted depending on which method yields better results for your specific data.


## Setup Instructions

### 1. Create a Virtual Environment (Recommended)

```bash
python -m venv venv
```

Activate the environment:

* **Windows:**

  ```bash
  venv\Scripts\activate
  ```
* **macOS/Linux:**

  ```bash
  source venv/bin/activate
  ```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Prepare `data.txt`

Place your reference text in a file named `data.txt` in the root directory.

### 4. Set API Key (If Using Google Gemini)

Set your `GOOGLE_API_KEY` as an environment variable.

* **Windows:**

  ```cmd
  set GOOGLE_API_KEY=your_actual_google_api_key_here
  ```
* **macOS/Linux:**

  ```bash
  export GOOGLE_API_KEY=your_actual_google_api_key_here
  ```

Alternatively, update the key directly in `app.py` if the application is configured to allow that.

## Running the Application

Start the Flask server:

```bash
python app.py
```

Access the application at: [http://127.0.0.1:5000/](http://127.0.0.1:5000/)

## Querying the `/rag` Endpoint

Send a POST request with your query in JSON format:

```bash
curl -X POST -H "Content-Type: application/json" -d '{"query":"YOUR_QUESTION_HERE"}' http://127.0.0.1:5000/rag
```

**Sample Response:**

```json
{
  "query": "YOUR_QUESTION_HERE",
  "answer": "The LLM's answer based on the retrieved context from data.txt..."
}
```



